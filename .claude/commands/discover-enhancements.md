AI-powered system improvement discovery and enhancement identification.

Discovery scope: $ARGUMENTS (optional: specific area or component to analyze)

Enhancement Discovery Process:
1. **System Analysis** (TELEMETRY-BASED DISCOVERY):
   - Scan codebase AND analyze performance metrics for improvement opportunities
   - Identify bottlenecks through benchmarking AND real-time telemetry data
   - Assess code quality using measurable metrics (complexity, coverage, etc.)
   - Review architectural patterns AND measure consistency against quality standards
   - Correlate code issues with production performance and error metrics

2. **Workflow Optimization**:
   - Examine n8n workflows for automation opportunities
   - Identify repetitive manual processes
   - Analyze agent coordination efficiency
   - Detect workflow bottlenecks and delays

3. **Code Quality Assessment**:
   - Find modules with low test coverage
   - Identify complex functions needing refactoring
   - Locate unused or deprecated code
   - Analyze error handling patterns

4. **Performance Opportunities** (BENCHMARK-DRIVEN IDENTIFICATION):
   - Database queries: measure execution times AND identify slow query patterns
   - Memory usage: analyze heap dumps AND track memory leak patterns over time
   - API response times: benchmark against SLA targets AND identify outliers
   - Caching strategies: measure hit rates AND analyze performance improvements
   - Resource efficiency: track CPU/memory usage trends AND identify optimization opportunities

5. **Security and Reliability**:
   - Vulnerability scanning and security gaps
   - Error handling and resilience improvements
   - Monitoring and observability enhancements
   - Backup and recovery procedures

6. **Developer Experience**:
   - Documentation gaps and clarity issues
   - Development tool and process improvements
   - Testing automation opportunities
   - CI/CD pipeline optimizations

7. **Self-Improvement Capabilities**:
   - AI agent coordination improvements
   - Automated enhancement detection
   - Self-healing and adaptation mechanisms
   - Knowledge management and learning systems

Discovery Methods (EVIDENCE-BASED ANALYSIS):
- Static code analysis WITH measurable quality metrics and trend tracking
- Performance metrics analysis using OpenTelemetry data and benchmarking
- Workflow execution data with statistical analysis and performance correlation
- Error log analysis with pattern recognition and root cause measurement
- Best practice comparison with quantifiable gap analysis and improvement metrics

Prioritization Criteria:
- Impact on system performance and reliability
- Development velocity and efficiency gains
- User experience and functionality improvements
- Maintenance burden reduction
- Security and compliance benefits

Output Format (METRICS-BACKED RECOMMENDATIONS):
- Ranked list with measurable impact scores and confidence intervals
- Impact estimation based on performance benchmarks and business metrics
- Effort estimation using historical data and complexity analysis
- Implementation recommendations with success criteria and validation methods
- APS process creation with measurable outcomes and telemetry requirements

Integration with AI Swarm:
- Create APS processes for significant enhancements
- Coordinate with appropriate agent roles
- Track improvement hypothesis and results
- Update system documentation with findings

The discovery process leverages AI analysis to continuously identify and prioritize system improvements, enabling autonomous enhancement and evolution.