Machine learning models exhibit various forms of bias that can significantly impact their performance and fairness. Algorithmic bias typically emerges from three primary sources: biased training data that doesn't represent the target population adequately, biased feature selection that amplifies existing societal prejudices, and biased evaluation metrics that favor certain demographic groups.

Training data bias occurs when historical data contains systematic discrimination or underrepresentation. For example, hiring algorithms trained on historical HR data may perpetuate gender or racial bias present in past hiring decisions. Feature selection bias happens when relevant demographic information is used directly or indirectly through proxy variables, leading to discriminatory outcomes.
